{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge of Dundee files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read all csv's and store in pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee/charge-sessions-june-sept.csv', '/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee/cpdata.csv', '/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee/cp-data-dec-mar-2018.csv', '/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee/cp-locations_enriched.csv', '/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee/cp-locations.csv', '/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee/cp-data-mar-may-2018.csv']\n"
     ]
    }
   ],
   "source": [
    "path =r'/home/ar/Google/DTU/Courses/Advanced business analytics/ABA - Project /Datasets/DUndee'#path of stored csv files\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two dataframes:\n",
    "1 - with all charge events from Dundee\n",
    "2 - with the enriched location information from Dundee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "\n",
    "for file in filenames:\n",
    "    if \"data\" in file or \"session\" in file:\n",
    "        data = pd.read_csv(file)\n",
    "        df = pd.concat([df,data])\n",
    "        \n",
    "loc=pd.DataFrame()\n",
    "for file in filenames:\n",
    "    if \"enriched\" in file:\n",
    "        data=pd.read_csv(file)\n",
    "        loc = pd.concat([loc, data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store a subset of location information on new dataframe to then merge it with the charge event merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate subset into new dataframe\n",
    "loc_restricted = loc[['CP ID', 'Latitude', 'Longitude']]\n",
    "#drop NaN values from loc_restricted and df dataframes\n",
    "df = df.dropna(subset=['CP ID'])\n",
    "loc_restricted=loc_restricted.dropna(subset=['CP ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df['CP ID'] = df['CP ID'].astype('Int64') #THis will give problems, when running all for first time change str by int. Then put str again. PRoblem with pandas converting eveything to floats at the beginnig and then when converting initially to string it causes all Long and latitude values to be NaN\n",
    "#loc_restricted['CP ID'] = loc_restricted['CP ID'].astype('Int64')\n",
    "\n",
    "df['CP ID'] = df['CP ID'].astype(str) #THis will give problems, when running all for first time change str by int. Then put str again. PRoblem with pandas converting eveything to floats at the beginnig and then when converting initially to string it causes all Long and latitude values to be NaN\n",
    "loc_restricted['CP ID'] = loc_restricted['CP ID'].astype(str)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(df, loc_restricted,  on='CP ID', how='left')#change to inner to keep only rows common on both df's by 'CP ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna(subset=['Start Time', 'End Time', 'Start Date', 'End Date'])\n",
    "\n",
    "# Combine Date and Time into a single datetime column\n",
    "merged_df['Start Date'] = pd.to_datetime(merged_df['Start Date'] + ' ' + merged_df['Start Time'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "merged_df['End Date'] = pd.to_datetime(merged_df['End Date'] + ' ' + merged_df['End Time'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['Start Time'])\n",
    "merged_df = merged_df.dropna(subset=['End Time'])\n",
    "\n",
    "merged_df = merged_df.drop('Start Time', axis=1)\n",
    "merged_df = merged_df.drop('End Time', axis=1)\n",
    "merged_df = merged_df.drop('Unnamed: 13', axis=1)\n",
    "merged_df = merged_df.drop('Unnamed: 14', axis=1)\n",
    "merged_df = merged_df.drop('Unnamed: 15', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column with total duration of charging event by substracting end date and start date\n",
    "merged_df['Total Duration (hh:mm:ss)'] = merged_df['End Date'] - merged_df['Start Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Charging event', 'User ID', 'CP ID', 'Connector', 'Start Date',\n",
       "       'End Date', 'Total kWh', 'Cost', 'Site', 'Group', 'Model', 'Latitude',\n",
       "       'Longitude', 'Total Duration (hh:mm:ss)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping={\n",
    "    'Charging event': 'Transaction ID',\n",
    "    'User ID': 'User ID',\n",
    "    'CP ID': 'EVSE ID',\n",
    "    'Connector': 'Plug Type',\n",
    "    #'Start Date': 'Start Date',\n",
    "    #'End Date': 'End date', \n",
    "    'Total kWh': 'Energy(kWh)', \n",
    "    'Cost': 'drop', \n",
    "    'Site':'Address 1', \n",
    "    'Group': 'drop',\n",
    "    'Model': 'Port Type', \n",
    "    #'Latitude':'Latitude',\n",
    "    #'Longitude':'Longitude'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns with value key drop\n",
    "columns_to_drop = [key for key, value in column_mapping.items() if value == 'drop']\n",
    "merged_df = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "# Replace name of columns\n",
    "merged_df = merged_df.rename(columns={k: v for k, v in column_mapping.items() if v != 'drop'})\n",
    "merged_df = merged_df.reindex(merged_df.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Transaction ID', 'User ID', 'EVSE ID', 'Plug Type', 'Start Date',\n",
       "       'End Date', 'Energy(kWh)', 'Address 1', 'Port Type', 'Latitude',\n",
       "       'Longitude', 'Total Duration (hh:mm:ss)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('Dundee_merged', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
